{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNcieatkMMxn"
      },
      "source": [
        "## **Building a Large Language Model from Scratch**\n",
        "Modern language models (like GPT-4) use transformers, a deep learning architecture that learns word relationships through self-attention. We’ll build a basic transformer-based model to understand how to build a large language model from scratch. The goal of our language model will be to predict the next word.\n",
        "\n",
        "Here are the six main components we’ll cover:\n",
        "\n",
        "- Tokenization\n",
        "- Embedding Layer\n",
        "- Positional Encoding\n",
        "- Self-Attention\n",
        "- Transformer Block\n",
        "- Full Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJ8Afz7TOGyW"
      },
      "source": [
        "##**Import Packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-27T05:02:08.151173Z",
          "iopub.status.busy": "2025-03-27T05:02:08.150731Z",
          "iopub.status.idle": "2025-03-27T05:02:10.485372Z",
          "shell.execute_reply": "2025-03-27T05:02:10.483925Z"
        },
        "id": "qOjF3-08FO3K"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qNYX5EoMW6e"
      },
      "source": [
        "## **Step 1: Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMRNlfAJFVSE"
      },
      "outputs": [],
      "source": [
        "def tokenize(text, vocab):\n",
        "    return [vocab.get(word, vocab[\"<UNK>\"]) for word in text.split()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhjr3lm7MiIh"
      },
      "source": [
        "##**Step 2: Embedding Layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S24U5s5gFZcb"
      },
      "outputs": [],
      "source": [
        "class Embedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(Embedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embedding(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoNpdVJnMmgL"
      },
      "source": [
        "##**Step 3: Positional Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a99iKh32JC2d"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embedding_dim, max_seq_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        pe = torch.zeros(max_seq_len, embedding_dim)\n",
        "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-math.log(10000.0) / embedding_dim))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(0), :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrTVj2HyMqjd"
      },
      "source": [
        "##**Step 4: Self-Attention**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtVQAIwSNH_a"
      },
      "outputs": [],
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.query = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.key = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.value = nn.Linear(embedding_dim, embedding_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        queries = self.query(x)\n",
        "        keys = self.key(x)\n",
        "        values = self.value(x)\n",
        "        scores = torch.bmm(queries, keys.transpose(1, 2)) / torch.sqrt(torch.tensor(x.size(-1), dtype=torch.float32))\n",
        "        attention_weights = torch.softmax(scores, dim=-1)\n",
        "        attended_values = torch.bmm(attention_weights, values)\n",
        "        return attended_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOB4h9sRMuGa"
      },
      "source": [
        "##**Step 5: Transformer Block**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sy9roGKbQyj6"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = SelfAttention(embedding_dim)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, embedding_dim)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
        "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attended = self.attention(x)\n",
        "        x = self.norm1(x + attended)\n",
        "        forwarded = self.feed_forward(x)\n",
        "        x = self.norm2(x + forwarded)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2EJ5kwZMxhp"
      },
      "source": [
        "##**Step 6: Full Language Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "He_2R8hjW4Hx"
      },
      "outputs": [],
      "source": [
        "class SimpleLLM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n",
        "        super(SimpleLLM, self).__init__()\n",
        "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
        "        self.positional_encoding = PositionalEncoding(embedding_dim)\n",
        "        self.transformer_blocks = nn.Sequential(*[TransformerBlock(embedding_dim, hidden_dim) for _ in range(num_layers)])\n",
        "        self.output = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = x.transpose(0, 1) # Transpose for positional encoding\n",
        "        x = self.positional_encoding(x)\n",
        "        x = x.transpose(0, 1) # Transpose back\n",
        "        x = self.transformer_blocks(x)\n",
        "        x = self.output(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRD6_xSqM2aX"
      },
      "source": [
        "##**Step 7: Training the Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKQv520OYpgz",
        "outputId": "d6fa4081-7874-442f-f68a-15272fe131da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 375.1386\n",
            "Epoch 20, Loss: 21.6977\n",
            "Epoch 40, Loss: 13.2326\n",
            "Epoch 60, Loss: 11.0227\n",
            "Epoch 80, Loss: 5.3205\n",
            "Epoch 100, Loss: 3.7815\n",
            "Epoch 120, Loss: 3.5573\n",
            "Epoch 140, Loss: 20.8091\n",
            "Epoch 160, Loss: 3.7572\n",
            "Epoch 180, Loss: 3.5122\n"
          ]
        }
      ],
      "source": [
        "vocab = {\n",
        "    \"hello\": 0, \"world\": 1, \"how\": 2, \"are\": 3, \"you\": 4,\n",
        "    \"<UNK>\": 5, \"good\": 6, \"morning\": 7, \"evening\": 8, \"night\": 9,\n",
        "    \"friend\": 10, \"nice\": 11, \"to\": 12, \"meet\": 13, \"learning\": 14,\n",
        "    \"AI\": 15, \"is\": 16, \"fun\": 17, \"great\": 18, \"awesome\": 19,\n",
        "    \"day\": 20, \"doing\": 21, \"today\": 22, \"hope\": 23, \"all\": 24,\n",
        "    \"well\": 25, \"enjoy\": 26, \"sunny\": 27, \"weather\": 28, \"coding\": 29,\n",
        "    \"Python\": 30, \"machine\": 31, \"data\": 32, \"science\": 33,\n",
        "    \"artificial\": 34, \"intelligence\": 35, \"deep\": 36, \"neural\": 37, \"network\": 38,\n",
        "    \"natural\": 39, \"language\": 40, \"processing\": 41, \"happy\": 42, \"birthday\": 43,\n",
        "    \"welcome\": 44, \"home\": 45, \"family\": 46, \"weekend\": 47, \"vacation\": 48,\n",
        "    \"study\": 49, \"new\": 50, \"projects\": 51, \"explore\": 52, \"knowledge\": 53,\n",
        "    \"reading\": 54, \"books\": 55, \"writing\": 56, \"articles\": 57, \"playing\": 58,\n",
        "    \"games\": 59, \"music\": 60, \"exercise\": 61, \"healthy\": 62, \"lifestyle\": 63,\n",
        "    \"delicious\": 64, \"food\": 65, \"travel\": 66, \"adventure\": 67, \"nature\": 68,\n",
        "    \"photography\": 69, \"technology\": 70, \"innovation\": 71, \"future\": 72\n",
        "}\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 32\n",
        "hidden_dim = 64\n",
        "num_layers = 2\n",
        "\n",
        "model = SimpleLLM(vocab_size, embedding_dim, hidden_dim, num_layers)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "data = [\n",
        "    \"hello world how are you\",\n",
        "    \"how are you hello world\",\n",
        "    \"good morning friend\",\n",
        "    \"nice to meet you\",\n",
        "    \"learning AI is fun\",\n",
        "    \"have a great day\",\n",
        "    \"hope you are doing well\",\n",
        "    \"AI is awesome\",\n",
        "    \"what are you doing today\",\n",
        "    \"good evening to all\",\n",
        "    \"enjoy the sunny weather\",\n",
        "    \"Python coding is exciting\",\n",
        "    \"machine learning is amazing\",\n",
        "    \"deep neural network is powerful\",\n",
        "    \"natural language processing is interesting\",\n",
        "    \"happy birthday to you\",\n",
        "    \"welcome home family\",\n",
        "    \"weekend vacation is relaxing\",\n",
        "    \"study new projects to explore knowledge\",\n",
        "    \"reading books is beneficial\",\n",
        "    \"writing articles improves skills\",\n",
        "    \"playing games is entertaining\",\n",
        "    \"music is refreshing\",\n",
        "    \"exercise maintains a healthy lifestyle\",\n",
        "    \"delicious food makes a great day\",\n",
        "    \"travel and adventure bring joy\",\n",
        "    \"nature photography is inspiring\",\n",
        "    \"technology and innovation shape the future\"\n",
        "]\n",
        "\n",
        "tokenized_data = [tokenize(sentence, vocab) for sentence in data]\n",
        "\n",
        "for epoch in range(200):\n",
        "    total_loss = 0\n",
        "    for sentence in tokenized_data:\n",
        "        for i in range(1, len(sentence)):\n",
        "            input_seq = torch.tensor(sentence[:i]).unsqueeze(0)\n",
        "            target = torch.tensor(sentence[i]).unsqueeze(0)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(input_seq)\n",
        "            loss = criterion(output[:, -1, :], target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "    if epoch % 20 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMSDyB-YM6X8"
      },
      "source": [
        "##**Step 8: Using the Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekguv82DYqXz",
        "outputId": "abbbd36d-cb7f-4bfe-fa54-363832bb7285"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence: AI is\n",
            "Input: AI is, Predicted: awesome\n"
          ]
        }
      ],
      "source": [
        "input_text = input(\"Enter a sentence: \")\n",
        "input_tokens = tokenize(input_text, vocab)\n",
        "input_tensor = torch.tensor(input_tokens).unsqueeze(0)\n",
        "output = model(input_tensor)\n",
        "predicted_token = torch.argmax(output[:, -1, :]).item()\n",
        "print(f\"Input: {input_text}, Predicted: {list(vocab.keys())[list(vocab.values()).index(predicted_token)]}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}